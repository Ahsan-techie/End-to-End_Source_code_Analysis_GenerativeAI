{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddeaea17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddad927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fe6fba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\source_code_analysis\\\\End-to-End_Source_code_Analysis_GenerativeAI\\\\research'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15352545",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir test_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f7ebec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "\n",
    "repo = Repo.clone_from(\n",
    "    \"https://github.com/Ahsan-techie/Medical_Chatbot_GenAI.git\",\n",
    "    to_path=repo_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56ee97dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GenericLoader.from_filesystem(\n",
    "    repo_path,\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".py\"],\n",
    "    parser=LanguageParser(language = Language.PYTHON, parser_threshold=500),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23f22a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c537594b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from flask import Flask, render_template, jsonify, request\\nfrom src.helper import download_hugging_face_embeddings\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom langchain_openai import OpenAI\\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom dotenv import load_dotenv\\nfrom src.prompt import *\\nimport os\\n\\napp = Flask(__name__)\\n\\nload_dotenv()\\n\\nPINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\\nOPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\\n\\nos.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\\n\\nembeddings = download_hugging_face_embeddings()\\n\\n\\nindex_name = \"medibot\"\\n\\n# Embed each chunk and upsert the embeddings into your Pinecone index.\\ndocsearch = PineconeVectorStore.from_existing_index(\\n    index_name=index_name, embedding=embeddings\\n)\\n\\nretriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\\n\\n\\nllm = OpenAI(temperature=0.4, max_tokens=500)\\nprompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system_prompt),\\n        (\"human\", \"{input}\"),\\n    ]\\n)\\n\\n\\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt)\\nrag_chain = create_retrieval_chain(retriever, question_answer_chain)\\n\\n\\n@app.route(\"/\")\\ndef index():\\n    return render_template(\"chat.html\")\\n\\n\\n@app.route(\"/get\", methods=[\"GET\", \"POST\"])\\ndef chat():\\n    msg = request.form[\"msg\"]\\n    input = msg\\n    print(input)\\n    response = rag_chain.invoke({\"input\": msg})\\n    print(\"Response : \", response[\"answer\"])\\n    return str(response[\"answer\"])\\n\\n\\nif __name__ == \"__main__\":\\n    app.run(host=\"0.0.0.0\", port=8080, debug=True)\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\setup.py', 'language': <Language.PYTHON: 'python'>}, page_content='from setuptools import setup, find_packages\\n\\nsetup(\\n    name=\"Generative AI Medical Bot\",\\n    version=\"1.0.0\",\\n    description=\"A chatbot for generating medical advice based on user inputs.\",\\n    author=\"Ahsan\",\\n    author_email=\"chahsan1393@gmail.com\",\\n    packages=find_packages(),\\n    install_requires=[],\\n)\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='from src.helper import load_pdf_file, text_split, download_hugging_face_embeddings\\nfrom pinecone.grpc import PineconeGRPC as Pinecone\\nfrom pinecone import ServerlessSpec\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom dotenv import load_dotenv\\nimport os\\n\\n\\nload_dotenv()\\n\\nPINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\\nos.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\\n\\n\\nextracted_data = load_pdf_file(data=\"Data/\")\\ntext_chunks = text_split(extracted_data)\\nembeddings = download_hugging_face_embeddings()\\n\\n\\npc = Pinecone(api_key=PINECONE_API_KEY)\\n\\nindex_name = \"medibot\"\\n\\n\\npc.create_index(\\n    name=index_name,\\n    dimension=384,\\n    metric=\"cosine\",\\n    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\\n)\\n\\n# Embed each chunk and upsert the embeddings into your Pinecone index.\\ndocsearch = PineconeVectorStore.from_documents(\\n    documents=text_chunks,\\n    index_name=index_name,\\n    embedding=embeddings,\\n)\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom pathlib import Path\\nimport logging\\n\\nlogging.basicConfig(level=logging.INFO, format=\"[%(asctime)s]: %(message)s\")\\n\\nlist_of_files = [\\n    \"src/__init__.py\",\\n    \"src/helper.py\",\\n    \"src/prompt.py\",\\n    \".env\",\\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\",\\n]\\n\\nfor filepath in list_of_files:\\n    filepath = Path(filepath)\\n    filedir, filename = os.path.split(filepath)\\n\\n    if filedir != \"\":\\n        os.makedirs(filedir, exist_ok=True)\\n        logging.info(f\"Creating directory: {filedir} for the file: {filename}\")\\n\\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\\n        with open(filepath, \"w\") as f:\\n            pass\\n            logging.info(f\"Creating file: {filename}\")\\n    else:\\n        logging.info(f\"File {filename} already exists\")\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='from langchain.document_loaders import PyPDFLoader, DirectoryLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\n\\ndef load_pdf_file(data):\\n    loader = DirectoryLoader(data, glob=\"*.pdf\", loader_cls=PyPDFLoader)\\n    documents = loader.load()\\n    return documents\\n\\n\\ndef text_split(extracted_data):\\n    text_slpitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\\n    text_chunks = text_slpitter.split_documents(extracted_data)\\n    return text_chunks\\n\\n\\ndef download_hugging_face_embeddings():\\n    embeddings = HuggingFaceEmbeddings(\\n        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\\n    )\\n    return embeddings\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}, page_content='system_prompt = (\\n    \"You are an assistant for question-answering tasks. \"\\n    \"Use the following pieces of retrieved context to answer \"\\n    \"the question. If you don\\'t know the answer, say that you \"\\n    \"don\\'t know. Use three sentences maximum and keep the \"\\n    \"answer concise.\"\\n    \"\\\\n\\\\n\"\\n    \"{context}\"\\n)\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb943e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed00be41",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language = Language.PYTHON,\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec37001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = documents_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82554d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from flask import Flask, render_template, jsonify, request\\nfrom src.helper import download_hugging_face_embeddings\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom langchain_openai import OpenAI\\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom dotenv import load_dotenv\\nfrom src.prompt import *\\nimport os\\n\\napp = Flask(__name__)\\n\\nload_dotenv()'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='load_dotenv()\\n\\nPINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\\nOPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\\n\\nos.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\\n\\nembeddings = download_hugging_face_embeddings()\\n\\n\\nindex_name = \"medibot\"\\n\\n# Embed each chunk and upsert the embeddings into your Pinecone index.\\ndocsearch = PineconeVectorStore.from_existing_index(\\n    index_name=index_name, embedding=embeddings\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\\n\\n\\nllm = OpenAI(temperature=0.4, max_tokens=500)\\nprompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system_prompt),\\n        (\"human\", \"{input}\"),\\n    ]\\n)\\n\\n\\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt)\\nrag_chain = create_retrieval_chain(retriever, question_answer_chain)\\n\\n\\n@app.route(\"/\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='def index():\\n    return render_template(\"chat.html\")\\n\\n\\n@app.route(\"/get\", methods=[\"GET\", \"POST\"])\\ndef chat():\\n    msg = request.form[\"msg\"]\\n    input = msg\\n    print(input)\\n    response = rag_chain.invoke({\"input\": msg})\\n    print(\"Response : \", response[\"answer\"])\\n    return str(response[\"answer\"])\\n\\n\\nif __name__ == \"__main__\":\\n    app.run(host=\"0.0.0.0\", port=8080, debug=True)'),\n",
       " Document(metadata={'source': 'test_repo\\\\setup.py', 'language': <Language.PYTHON: 'python'>}, page_content='from setuptools import setup, find_packages\\n\\nsetup(\\n    name=\"Generative AI Medical Bot\",\\n    version=\"1.0.0\",\\n    description=\"A chatbot for generating medical advice based on user inputs.\",\\n    author=\"Ahsan\",\\n    author_email=\"chahsan1393@gmail.com\",\\n    packages=find_packages(),\\n    install_requires=[],\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='from src.helper import load_pdf_file, text_split, download_hugging_face_embeddings\\nfrom pinecone.grpc import PineconeGRPC as Pinecone\\nfrom pinecone import ServerlessSpec\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom dotenv import load_dotenv\\nimport os\\n\\n\\nload_dotenv()\\n\\nPINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\\nos.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY'),\n",
       " Document(metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='extracted_data = load_pdf_file(data=\"Data/\")\\ntext_chunks = text_split(extracted_data)\\nembeddings = download_hugging_face_embeddings()\\n\\n\\npc = Pinecone(api_key=PINECONE_API_KEY)\\n\\nindex_name = \"medibot\"\\n\\n\\npc.create_index(\\n    name=index_name,\\n    dimension=384,\\n    metric=\"cosine\",\\n    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='# Embed each chunk and upsert the embeddings into your Pinecone index.\\ndocsearch = PineconeVectorStore.from_documents(\\n    documents=text_chunks,\\n    index_name=index_name,\\n    embedding=embeddings,\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom pathlib import Path\\nimport logging\\n\\nlogging.basicConfig(level=logging.INFO, format=\"[%(asctime)s]: %(message)s\")\\n\\nlist_of_files = [\\n    \"src/__init__.py\",\\n    \"src/helper.py\",\\n    \"src/prompt.py\",\\n    \".env\",\\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\",\\n]\\n\\nfor filepath in list_of_files:\\n    filepath = Path(filepath)\\n    filedir, filename = os.path.split(filepath)'),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='if filedir != \"\":\\n        os.makedirs(filedir, exist_ok=True)\\n        logging.info(f\"Creating directory: {filedir} for the file: {filename}\")\\n\\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\\n        with open(filepath, \"w\") as f:\\n            pass\\n            logging.info(f\"Creating file: {filename}\")\\n    else:\\n        logging.info(f\"File {filename} already exists\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='from langchain.document_loaders import PyPDFLoader, DirectoryLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\n\\ndef load_pdf_file(data):\\n    loader = DirectoryLoader(data, glob=\"*.pdf\", loader_cls=PyPDFLoader)\\n    documents = loader.load()\\n    return documents'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='def text_split(extracted_data):\\n    text_slpitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\\n    text_chunks = text_slpitter.split_documents(extracted_data)\\n    return text_chunks\\n\\n\\ndef download_hugging_face_embeddings():\\n    embeddings = HuggingFaceEmbeddings(\\n        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\\n    )\\n    return embeddings'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}, page_content='system_prompt = (\\n    \"You are an assistant for question-answering tasks. \"\\n    \"Use the following pieces of retrieved context to answer \"\\n    \"the question. If you don\\'t know the answer, say that you \"\\n    \"don\\'t know. Use three sentences maximum and keep the \"\\n    \"answer concise.\"\\n    \"\\\\n\\\\n\"\\n    \"{context}\"\\n)')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3158b50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d661549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1568851",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26a0c9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prime Laptop\\AppData\\Local\\Temp\\ipykernel_17172\\805463212.py:1: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(disallowed_special=())\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de4de7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(\n",
    "    persist_directory=\"./chroma_db\",  # directory where the vectorstore is persisted\n",
    "    collection_name=\"code-snippets\",  # collection name used when storing your documents\n",
    "    embedding_function=embeddings,  # embedding function from OpenAIEmbeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3f41276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings computed and stored in Chroma!\n"
     ]
    }
   ],
   "source": [
    "vectorstore.add_documents(codes)\n",
    "print(\"Embeddings computed and stored in Chroma!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f82b92ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f9dbd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\",output_key='answer', return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5b77eeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    memory=memory,\n",
    "    return_source_documents=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "26ece68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.output_key = 'answer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b0543c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is download_hugging_face_embeddings funtion?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "26d693e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "def text_split(extracted_data):\n",
      "    text_slpitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
      "    text_chunks = text_slpitter.split_documents(extracted_data)\n",
      "    return text_chunks\n",
      "\n",
      "\n",
      "def download_hugging_face_embeddings():\n",
      "    embeddings = HuggingFaceEmbeddings(\n",
      "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
      "    )\n",
      "    return embeddings\n",
      "\n",
      "from src.helper import load_pdf_file, text_split, download_hugging_face_embeddings\n",
      "from pinecone.grpc import PineconeGRPC as Pinecone\n",
      "from pinecone import ServerlessSpec\n",
      "from langchain_pinecone import PineconeVectorStore\n",
      "from dotenv import load_dotenv\n",
      "import os\n",
      "\n",
      "\n",
      "load_dotenv()\n",
      "\n",
      "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\n",
      "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
      "\n",
      "\n",
      "extracted_data = load_pdf_file(data=\"Data/\")\n",
      "text_chunks = text_split(extracted_data)\n",
      "embeddings = download_hugging_face_embeddings()\n",
      "\n",
      "\n",
      "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
      "\n",
      "index_name = \"medibot\"\n",
      "\n",
      "\n",
      "pc.create_index(\n",
      "    name=index_name,\n",
      "    dimension=384,\n",
      "    metric=\"cosine\",\n",
      "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
      ")\n",
      "\n",
      "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
      "docsearch = PineconeVectorStore.from_documents(\n",
      "    documents=text_chunks,\n",
      "    index_name=index_name,\n",
      "    embedding=embeddings,\n",
      ")\n",
      "\n",
      "\n",
      "extracted_data = load_pdf_file(data=\"Data/\")\n",
      "text_chunks = text_split(extracted_data)\n",
      "embeddings = download_hugging_face_embeddings()\n",
      "\n",
      "\n",
      "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
      "\n",
      "index_name = \"medibot\"\n",
      "\n",
      "\n",
      "pc.create_index(\n",
      "    name=index_name,\n",
      "    dimension=384,\n",
      "    metric=\"cosine\",\n",
      "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
      ")\n",
      "\n",
      "load_dotenv()\n",
      "\n",
      "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\n",
      "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
      "\n",
      "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
      "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
      "\n",
      "embeddings = download_hugging_face_embeddings()\n",
      "\n",
      "\n",
      "index_name = \"medibot\"\n",
      "\n",
      "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
      "docsearch = PineconeVectorStore.from_existing_index(\n",
      "    index_name=index_name, embedding=embeddings\n",
      ")\n",
      "Human: what is download_hugging_face_embeddings funtion?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The `download_hugging_face_embeddings` function is a function that downloads a specific pre-trained model for generating embeddings from the Hugging Face model hub. In this case, the model being downloaded is \"sentence-transformers/all-MiniLM-L6-v2\".\n"
     ]
    }
   ],
   "source": [
    "result = qa.invoke({\"question\":question})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0136cffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "system: The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential. The human asks what the `download_hugging_face_embeddings` function is. The AI explains that it is a function that downloads a specific pre-trained model for generating embeddings from the Hugging Face model hub, specifically the \"sentence-transformers/all-MiniLM-L6-v2\" model.\n",
      "Follow Up Input: what is load_pdf_file funtion?\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
      "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
      "from langchain.embeddings import HuggingFaceEmbeddings\n",
      "\n",
      "\n",
      "def load_pdf_file(data):\n",
      "    loader = DirectoryLoader(data, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
      "    documents = loader.load()\n",
      "    return documents\n",
      "\n",
      "from src.helper import load_pdf_file, text_split, download_hugging_face_embeddings\n",
      "from pinecone.grpc import PineconeGRPC as Pinecone\n",
      "from pinecone import ServerlessSpec\n",
      "from langchain_pinecone import PineconeVectorStore\n",
      "from dotenv import load_dotenv\n",
      "import os\n",
      "\n",
      "\n",
      "load_dotenv()\n",
      "\n",
      "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\n",
      "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
      "\n",
      "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
      "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
      "from langchain.embeddings import HuggingFaceEmbeddings\n",
      "\n",
      "\n",
      "def load_pdf_file(data):\n",
      "    loader = DirectoryLoader(data, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
      "    documents = loader.load()\n",
      "    return documents\n",
      "\n",
      "\n",
      "def text_split(extracted_data):\n",
      "    text_slpitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
      "    text_chunks = text_slpitter.split_documents(extracted_data)\n",
      "    return text_chunks\n",
      "\n",
      "\n",
      "def download_hugging_face_embeddings():\n",
      "    embeddings = HuggingFaceEmbeddings(\n",
      "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
      "    )\n",
      "    return embeddings\n",
      "\n",
      "\n",
      "from src.helper import load_pdf_file, text_split, download_hugging_face_embeddings\n",
      "from pinecone.grpc import PineconeGRPC as Pinecone\n",
      "from pinecone import ServerlessSpec\n",
      "from langchain_pinecone import PineconeVectorStore\n",
      "from dotenv import load_dotenv\n",
      "import os\n",
      "\n",
      "\n",
      "load_dotenv()\n",
      "\n",
      "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\n",
      "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
      "\n",
      "\n",
      "extracted_data = load_pdf_file(data=\"Data/\")\n",
      "text_chunks = text_split(extracted_data)\n",
      "embeddings = download_hugging_face_embeddings()\n",
      "\n",
      "\n",
      "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
      "\n",
      "index_name = \"medibot\"\n",
      "\n",
      "\n",
      "pc.create_index(\n",
      "    name=index_name,\n",
      "    dimension=384,\n",
      "    metric=\"cosine\",\n",
      "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
      ")\n",
      "\n",
      "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
      "docsearch = PineconeVectorStore.from_documents(\n",
      "    documents=text_chunks,\n",
      "    index_name=index_name,\n",
      "    embedding=embeddings,\n",
      ")\n",
      "\n",
      "Human: What is the `load_pdf_file` function?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The `load_pdf_file` function is a function that takes in a directory path as input, loads PDF files from that directory, and returns the extracted documents from those PDF files. It uses a `DirectoryLoader` with a `PyPDFLoader` to load the PDF files.\n"
     ]
    }
   ],
   "source": [
    "question = \"what is load_pdf_file funtion?\"\n",
    "result = qa.invoke({\"question\":question})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98436707",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
